%%%%%%%%%%%%%%%%%%%%%%% file template.tex %%%%%%%%%%%%%%%%%%%%%%%%%
%
% This is a general template file for the LaTeX package SVJour3
% for Springer journals.          Springer Heidelberg 2010/09/16
%
% Copy it to a new file with a new name and use it as the basis
% for your article. Delete % signs as needed.
%
% This template includes a few options for different layouts and
% content for various journals. Please consult a previous issue of
% your journal as needed.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% First comes an example EPS file -- just ignore it and
% proceed on the \documentclass line
% your LaTeX will extract the file if required
%\begin{filecontents*}{example.eps}
%!PS-Adobe-3.0 EPSF-3.0
%%BoundingBox: 19 19 221 221
%%CreationDate: Mon Sep 29 1997
%%Creator: programmed by hand (JK)
%%EndComments
%gsave
%newpath
%  20 20 moveto
%  20 220 lineto
% 220 220 lineto
%  220 20 lineto
%closepath
%2 setlinewidth
%gsave
%  .4 setgray fill
%grestore
%stroke
%grestore
%\end{filecontents*}
%
%\RequirePackage{fix-cm}
%
%\documentclass{svjour3}                     % onecolumn (standard format)
\documentclass[smallcondensed]{svjour3}     % onecolumn (ditto)
%% Text Formatting
%% Manuscripts should be submitted in LaTeX. Please use Springer�s 
%% LaTeX macro package and choose the formatting option �smallcondensed�.
%\documentclass[smallextended]{svjour3}       % onecolumn (second format)
%\documentclass[twocolumn]{svjour3}          % twocolumn
%
\smartqed  % flush right qed marks, e.g. at end of proof
%
\usepackage{graphicx}
%
% \usepackage{mathptmx}      % use Times fonts if available on your TeX system
%
% insert here the call for the packages your document requires
%\usepackage{latexsym}
% etc.
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{float}
\usepackage{color}
\usepackage{algorithm}
\usepackage{algorithmic}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
%
% please place your own definitions here and don't use \def but
% \newcommand{}{}
%
% Insert the name of "your journal" with
%\journalname{Numerical Algorithms} %Adv Comput Math
\journalname{Springer International Series of Numerical Mathematics??}
%
\begin{document}

\title{Balanced truncation for parametric linear systems
	using interpolation of gramians: a comparison of
	linear algebraic and geometric approaches}
%\subtitle{Application for a research assistant position at ICTEAM, UC Louvain}
%\thanks{}
%\thanks{Grants or other notes
%about the article that should go on the front page should be
%placed here. General acknowledgments should be placed at the end of the article.}
%\subtitle{Do you have a subtitle?\\ If so, write it here}

\titlerunning{Algebraic and Geometric interpolation of gramians for PMOR}        % if too long for running head

\author{SGMSA}  %etc.}

%\authorrunning{Short form of author list} % if too long for running head

%\institute{N. T. Son \at
%             INMA, ICTEAM, Universit/'e catholique de Louvain, 
%             Avenue Georges Lemaître 4-6/L4.05.01,\\
%             1348 Louvain-la-Neuve
%             \\
%              Tel: +32 10 47 80 10 \\
%             \email{thanh.son.nguyen@uclouvain.be}  \\%  if needed
%}

%\date{Received: date / Accepted: date}
% The correct dates will be entered by the editor


\maketitle

\begin{abstract}
In balanced truncation model order reduction, one has to solve a pair of Lyapunov equations for the two gramians and uses them for constructing a reduced-order
model. Although advances in solving such equations have been made, it is still the most
expensive step in this reduction method. For systems that depend on parameters, parametric model order reduction has to deal with the dependence on  parameters simultaneously with approximation of the input-output behavior of the full-order system. The
use of interpolation in parametric model order reduction has become popular. Nevertheless, interpolation of gramians is rarely mentioned, most probably due to the restriction to symmetric
positive semi-definite matrices. In this talk, we will present two approaches for interpolating
these structured matrices which are based on linear algebra and a recently developed Riemannian geometry. The result is then utilized in constructing parametric reduced-order
systems. Their numerical performances are compared on different models
\keywords{
Parametric model order reduction \and Balanced truncation \and Interpolation \and Gramians \and Riemannian matrix manifold \and Symmetric positive semi-definite matrices of fixed rank 
}
% \PACS{PACS code1 \and PACS code2 \and more}
\subclass{
%15A22    % Matrix pencils 
%\and
 65D05 % Interpolation
\and 65F30 % Other matrix algorithms
\and 93C05 % Control systems, Linear systems
%\and 94C % Circuits, networks
\and notdoneyet
}
\end{abstract}


\section{Introduction}\label{Sec:Intro}
The need for increasingly accurate simulations in science and
technology results in large-scale mathematical models. Simulation of those systems is usually time-consuming or even infeasible, especially with limited computer resources. Model order reduction (MOR) is well known as a tool to
deal with such problems. Founded and continuously developed already for a couple of decades, this field is still getting attraction due to the fact that many complicated or large problems have not been considered and many advanced methods have not been invoked. 

In many cases, the full order model (FOM) depends on parameters. The reduced-order model (ROM), preferably parameter-dependent as well, is therefore required to approximate the FOM on a given parameter domain. This problem, so-called parametric MOR (PMOR), has been addressed by various approaches that are based on Krylov subspaces [cite], interpolation, optimization [cite], just to name a few. The reader is referred to the survey \cite{BennGW15} for more details. Of our interest in this report is the methods that use interpolation for the linear parametric system of the form
\begin{equation}
\label{eq:psys}
\arraycolsep=2pt
\begin{array}{rcl}
E(\mu)\dot{x}(t,\mu) & = &A(\mu)x(t,\mu)+B(\mu)u(t),\\
y(t,\mu) & = & C(\mu)x(t,\mu),
\end{array}
\end{equation}
where $E(\mu)$, $A(\mu) \in \mathbb{R}^{n\times n}$, $B(\mu) \in \mathbb{R}^{n\times m}$, 
$C(\mu) \in \mathbb{R}^{p\times n}$ with $p,m \ll n$, and $\mu\in\Omega\subset \mathbb{R}^d$. We assume that the matrix 
$E(\mu)$ is nonsingular and all eigenvalues of the pencil $\lambda E(\mu)-A(\mu)$ have negative real part for all $\mu\in\Omega$. The goal is to approximate system \eqref{eq:psys} with a parametric model
\begin{equation}
\label{eq:redpsys}
\arraycolsep=2pt
\begin{array}{rcl}
\tilde{E}(\mu)\dot{\tilde{x}}(t,\mu) & =\tilde{A}(\mu)\tilde{x}(t,\mu)+\tilde{B}(\mu)u(t),\\
\tilde{y}(t,\mu) & =\tilde{C}(\mu)\tilde{x}(t,\mu),
\end{array}
\end{equation}
where $\tilde{E}(\mu)$, $\tilde{A}(\mu) \in \mathbb{R}^{r\times r}$, $\tilde{B}(\mu)\in\mathbb{R}^{r\times m}$, 
$\tilde{C}(\mu) \in \mathbb{R}^{p\times r}$ and $r \ll n$. 

The idea of interpolation is straightforward. On a given grid $\mu_i$, $i = 1, \dots, N$ in the parameter domain $\Omega$, one computes a ROM associated with each $\mu_i$. These ROMs can be computed using any MOR method for non-parametric models \cite{Anto05} and characterized by either their  projection subspaces, coefficient matrices, or transfer functions. Then, they are interpolated using standard methods. These topics have been discussed intensively in many publications with applications to various fields, see, e.g., \cite{AmsaF08,BaurB09,PanzMEL10,DegrVW10,AmsaF11,Son12,SonS15}. Each of them has its own strength and acts well in some specific applications but fails to be superior to the others. 

When balanced truncation \cite{Moor81} is used,  one has to solve a pair of Lyapunov equations for the two gramians. Although advances in solving such equations have been made, it is still the most expensive step in this reduction method. Therefore, any interpolation method that can circumvent this step is of interest. We suggest to interpolate the solution of these equations, so-called gramians. It is noteworthy that in large-scale setting, one should never work with full-rank solution matrices. Fortunately, in many practical cases, Lyapunov equaions accept low-rank symmetric positive semi-definite (spsd) approximation \cite{Penz00b,AntoSZ02}. It not only makes the computation more efficient but also enables the squaring procedure in balanced truncation \cite{TombP87}. A resulting difficulty arises in the interpolation approach is that the interpolant is also expected to be low-rank spsd. That is, the spsd property must be preserved during the interpolation. To this end, we propose in this paper two approaches. In the first one, we first invoke some positive interpolation scheme to preserve the semi-definiteness and then use some compression technique to keep the interpolant low-rank. After that, an offline-online decomposition is used to integrate this step into the whole reduction process that accelerates the online stage. We refer to this as linear algebraic approach. The second one is based completely on differential geometry, so the name. It was shown in \cite{VandAV09,MassA18} that the set of spsd matrices of fixed rank can be turned into a Riemannian manifold by equipping it with a differential structure. As a result, if all solutions of the Lyapunov equations at grid points are approximated by spsd matrices of a prescribed low rank, we then encounter interpolation on a Riemannian manifold. 

The rest of the paper is organized as follows. In section~\ref{Sec:BT_standard interpolation} we first briefly recall the squaring procedure for balancing equation and then present in detail how interpolate the gramians while preserve its low-rank semi-positiveness and how to prepare data in the offline step so that we can speed up the online step. A quotient geometry for the Riemannian manifold of spsd matrices of fixed rank is constructed in the first part of section~\ref{Sec:Manifold}. We then explain in detail how to interpolate on this manifold using the tools just developed. We also discuss the possibility of using a embedded geometry  for this talk in this section. The proposed approaches are illustrated by numerical examples in section~\ref{Sec:NumerExam} in which we also compare their behaviors and numerical efficiency. Conclusion is given in section~\ref{Sec:Concl}. 

Throughout this paper, we will use ... for the nations.





\section{Brief balanced truncation for parametric linear systems and standard interpolation}\label{Sec:BT_standard interpolation}
\subsection{Balanced truncation}
As other projection-based method, a balancing projection for system \eqref{eq:psys} must be constructed. To this end, one has to solve a pair of the generalized Lyapunov equations
\begin{eqnarray}
\label{eq:LyapContr}
E(\mu)P(\mu)A^T\!(\mu) + A(\mu)P(\mu)E^T\!(\mu) & = & -B(\mu)B^T\!(\mu), \\
\label{eq:LyapObser}
E^T(\mu)Q(\mu)A(\mu) + A^T(\mu)Q(\mu)E(\mu) & = & -C^T(\mu)C(\mu),
\end{eqnarray}
for the \textit{controllability Gramian} $P(\mu)$ and the \textit{observability Gramian} $Q(\mu)$. In practice, 
these Gramians are computed in the factored form 
$$
P(\mu) = X(\mu)X^T(\mu), \qquad Q(\mu) = Y(\mu)Y^T(\mu)
$$ 
with $X(\mu)\in\mathbb{R}^{n\times k_c}$ and $Y(\mu)\in\mathbb{R}^{n\times k_o}$. One can show that eigenvalues of the matrix $P(\mu)E^T(\mu)Q(\mu)E(\mu)$ are real and non-negative. The square roots of the eigenvalues of this matrix $\sigma_1(\mu) \geq \cdots \geq \sigma_{n}(\mu)\geq 0$ are called the \textit{Hankel singular values} of system \eqref{eq:psys}. Consider the singular value decomposition 
\begin{align}
\label{eq:SVD}
Y^T(\mu)E(\mu)X(\mu) = [U_1(\mu)\enskip U_0(\mu)]\begin{bmatrix}
\Sigma_1(\mu) & 0\\ 0 & \Sigma_0(\mu)
\end{bmatrix} [V_1(\mu)\enskip V_0(\mu)]^T,
\end{align}
where $[U_1(\mu)\enskip U_0(\mu)]$ and $[V_1(\mu)\enskip V_0(\mu)]$ are orthogonal, and
$$
\Sigma_1(\mu) = \mbox{diag}(\sigma_1(\mu),\ldots,\sigma_r(\mu)), \quad 
\Sigma_0(\mu) = \mbox{diag}(\sigma_{r+1}(\mu),\ldots,\sigma_{k_{co}}(\mu))
$$
with $k_{co}=\min(k_c,k_o)$.
Then the reduced-order model is computed by projection 
\begin{equation}\label{eq:RedMatr}
\begin{array}{ll}
\tilde{E}(\mu) = W^T(\mu)E(\mu)T(\mu),& \quad \tilde{A}(\mu) = W^T(\mu)A(\mu)T(\mu), \\
\tilde{B}(\mu) = W^T(\mu)B(\mu), & \quad \tilde{C} = C(\mu)T(\mu),
\end{array}
\end{equation}
where the projection matrices are given by
\begin{equation}
W(\mu) = Y(\mu)U_1(\mu) \Sigma_1^{-1/2}(\mu), \qquad T(\mu) = X(\mu)V_1(\mu)\Sigma_1^{-1/2}(\mu).
\label{eq:WT}
\end{equation}
The error of the approximation is shown to satisfy 
\begin{equation*}%\label{S2ErrBound}
\|H(\cdot,\mu)-\tilde{H}(\cdot,\mu)\|_{\mathcal{H}_\infty} \leq 2\bigl(\sigma_{r+1}(\mu) + \cdots + \sigma_{k_{co}}(\mu)\bigr),
\end{equation*}
where 
\begin{align*}
H(s,\mu)& =C(\mu)(sE(\mu)-A(\mu))^{-1}B(\mu), \\
\tilde{H}(s,\mu)&=\tilde{C}(\mu)(s\tilde{E}(\mu)-\tilde{A}(\mu))^{-1}\tilde{B}(\mu)
\end{align*}
are the transfer functions of systems \eqref{eq:psys} and \eqref{eq:redpsys}, respectively. 
\subsection{Interpolation of gramians for parametric model order reduction}
To wit, 
on a~chosen grid $\mu_1,\ldots,\mu_k \in \Omega$, we solve the equations (\ref{eq:LyapContr}) and (\ref{eq:LyapObser}) 
for $P_j=X_j^{}X_j^T\in\mathcal{S}_+(k_c,n)$ and $Q_j=Y_j^{}Y_j^T\in\mathcal{S}_+(k_o,n)$, $j=1,\ldots,q$. Then the parameter-dependent Gramians can be obtained in the form
\begin{align*}
P(\mu) =R\bigl(P_*, \sum_{j=1}^q w_j(\mu) L(P_*,P_j)\bigr)=U(\mu)Z^{-1}\!(\mu)U^T(\mu), \\
Q(\mu) =R\bigl(Q_*, \sum_{j=1}^q w_j(\mu) L(Q_*,Q_j)\bigr)=V(\mu)R^{-1}\!(\mu)V^T(\mu)
\end{align*}
using the interpolation method presented in Section~\ref{sec:Inter}, where the contact points 
$P_*=\hat{X}\hat{X}^T\in\mathcal{S}_+(k_c,n)$ and $Q_*=\hat{Y}\hat{Y}^T\in\mathcal{S}_+(k_o,n)$ are determined by averaging as described in Section~\ref{sec:Aver}. Using \eqref{eq:UPhi} the factors of the Gramians can be represented as
\begin{equation}
\arraycolsep=2pt
\begin{array}{rcll}
U(\mu) & = & \sum\limits_{j=1}^q w_j(\mu) U_j, & \qquad U_j=X_j(X_j^T \hat{X}),\\
\Phi(\mu) & = & \sum\limits_{j=1}^q w_j(\mu) \Phi_j, & \qquad \Phi_j=(X_j^T \hat{X})^T(X_j^T \hat{X}),
\end{array}
\label{eq:UPhimu}
\end{equation}
and 
\begin{equation}
\arraycolsep=2pt
\begin{array}{rcll}
V(\mu) & = & \sum\limits_{j=1}^q w_j(\mu) V_j, & \qquad V_j=Y_j(Y_j^T \hat{Y}), \\
\Psi(\mu) & = & \sum\limits_{j=1}^q w_j(\mu) \Psi_j, & \qquad \Psi_j=(Y_j^T \hat{Y})^T(Y_j^T \hat{Y}).
\end{array}
\label{eq:VPsimu}
\end{equation}
Considering the Cholesky factorizations 
\begin{equation}
\Phi(\mu)=C_{\Phi}^{}(\mu)C_{\Phi}^T(\mu), \qquad
\Psi(\mu)=C_{\Psi}^{}(\mu)C_{\Psi}^T(\mu),
\label{eq:Chol}
\end{equation}
we rewrite the Gramians $P(\mu)$ and $Q(\mu)$ as 
\begin{equation}
\arraycolsep=2pt
\begin{array}{ll}
P(\mu)=X(\mu)X^T(\mu), & \qquad X(\mu) =U(\mu)C_{\Phi}^{-T}(\mu),\\
Q(\mu)=Y(\mu)Y^T(\mu), & \qquad Y(\mu) =V(\mu)C_{\Psi}^{-T}(\mu).
\end{array}
\label{eq:PQmu}
\end{equation}
After the interpolation step, we still have to proceed steps \eqref{eq:SVD} and \eqref{eq:RedMatr}) to get the reduced-order models; computation still involves large matrices. Due to the simulation requirement of parameter dependent systems, the online stage should have the computational complexity independent of $n$. We propose a method which decomposes the procedure into two stages, so-called offline-online decomposition. The advantage of this algorithm is that in the offline stage, expensive and time consuming computations can be done and the parameter-independent data can be stored for the online stage. The online stage, counted from the moment when a value of $\mu$ is given till the moment that the reduced-order system is computed, has computational complexity independent of $n$ and therefore can be used in real time \cite{Son13}. 

\subsection{Offline-online decomposition}

To enable the online-offline decomposition, we need an assumption that the system matrices in (\ref{eq:psys}) 
depend affinely on the parameter $\mu$. More precisely,
\begin{align*}%\label{S3AffDep}
&E(\mu) = \sum\limits_{i=1}^{q_E}f_i^E(\mu)E_i,\quad A(\mu) = \sum\limits_{i=1}^{q_A}f_i^A(\mu)A_i,\\
&B(\mu) = \sum\limits_{i=1}^{q_B}f_i^B(\mu)B_i, \quad C(\mu) = \sum\limits_{i=1}^{q_C}f_i^C(\mu)C_i,
\end{align*}
where $q_E, q_A, q_B$ and $q_C$ are small, and the evaluation of $f_i^E,f_i^A,f_i^B$ and $f_i^C$ is  cheap. When the interpolated Gramians \eqref{eq:PQmu} available, we have
\begin{align}\notag
Y^T(\mu)E(\mu)X(\mu) &= C_{\Psi}^{-1}(\mu)\Bigl(\sum\limits_{j=1}^q w_j(\mu) V_j^T\Bigr)\Bigl(\sum\limits_{i=1}^{q_E}f_i^E(\mu)E_i\Bigr)\Bigl(\sum\limits_{l=1}^q w_l(\mu) U_l\Bigr)C_{\Phi}^{-T}(\mu)\\
& = C_{\Psi}^{-1}(\mu)\Bigl(\sum\limits_{j,l=1}^{q}\sum\limits_{i=1}^{q_E} w_j(\mu)w_l(\mu)f_i^E(\mu) V_j^TE_iU_l^{}\Bigr)C_{\Phi}^{-T}(\mu).
\label{eq:pSVD}
\end{align}
Computing the SVD (\ref{eq:SVD}) of this matrix, the projection matrices are determined as in \eqref{eq:WT}. 
The reduced-order matrices \eqref{eq:RedMatr} are then computed on the same manner as the product (\ref{eq:pSVD}):
\begin{equation}
\arraycolsep=2pt
\begin{array}{rcl}
\tilde{E}(\mu) & = & %W^T(\mu)E(\mu)T(\mu)= 
F(\mu)\Bigl(\sum\limits_{j,l=1}^{q}\sum\limits_{i=1}^{q_E} w_j(\mu)w_l(\mu)f_i^E(\mu) V_j^TE_iU_l^{}\Bigr)G(\mu),\\
\tilde{A}(\mu) & = & F(\mu)\Bigl(\sum\limits_{j,l=1}^{q}\sum\limits_{i=1}^{q_A} w_j(\mu)w_l(\mu)f_i^A(\mu) V_j^T A_iU_l^{}\Bigr)G(\mu),\\
\tilde{B}(\mu) & = & F(\mu)\Bigl(\sum\limits_{j=1}^{q}\sum\limits_{i=1}^{q_B} w_j(\mu)f_i^B(\mu) V_j^T B_i\Bigr),\\
\tilde{C}(\mu) & = & \Bigl(\sum\limits_{l=1}^{q}\sum\limits_{i=1}^{q_C} w_l(\mu)f_i^C(\mu) C_iU_l^{}\Bigr)G(\mu),
\end{array}
\label{eq:RedMatrPar}
\end{equation}
where $F(\mu)=\Sigma_1^{-1/2}(\mu)U_1^T(\mu)C_{\Psi}^{-1}(\mu)$ and $G(\mu)=C_{\Phi}^{-T}(\mu)V_1(\mu) \Sigma_1^{-1/2}(\mu)$. Note that all parameter-independent matrices in \eqref{eq:pSVD} and \eqref{eq:RedMatrPar} can be pre-computed and stored in the offline stage and used then in the online stage to compute \eqref{eq:pSVD} and \eqref{eq:RedMatrPar} for any value $\mu\in\Omega$. 
Based on the above analysis, the offline-online procedure is given as follows.

{\bf Offline:} \begin{itemize}
	\item For $\mu_1,\ldots,\mu_q\in\Omega$, solve the Lyapunov equations \eqref{eq:LyapContr} and \eqref{eq:LyapObser} for $P_j= X_j^{}X_j^T$ and $Q_j= Y_j^{}Y_j^T$, $j = 1,\ldots,q$, respectively, using, for example, the alternating direction implicit method \cite{,};
	
	\item compute the contact points $P_*=\hat{X}\hat{X}^T$ and $Q_*=\hat{Y}\hat{Y}^T$ using the averaging procedure in Algorithm~\ref{alg:averaging};
	
	\item compute and store all parameter-independent matrices $U_j$, $\Phi_j$ and $V_j$, $\Psi_j$ as in \eqref{eq:UPhimu} and \eqref{eq:VPsimu};
	
	\item compute and store all parameter-independent matrices in \eqref{eq:pSVD} and \eqref{eq:RedMatrPar}.
\end{itemize}
{\bf Online:} For given $\mu\in\Omega$, \hfill Costs:
\begin{itemize}
	\item evaluate $w_j(\mu)$ for $j=1,\ldots, q$;
	
	\item assemble the matrices $\Phi(\mu)$ and $\Psi(\mu)$ as in \eqref{eq:UPhimu} and \eqref{eq:VPsimu};
	\hfill $2q(k_c^2+k_o^2)$
	
	\item compute the Cholesky factorizations \eqref{eq:Chol}; \hfill $(k_c^3+k_o^3)/6$
	
	\item assemble the matrix \eqref{eq:pSVD}; \hfill $2q^2q_Ek_ck_o+k_c^2k_o+k_c k_o^2$
	
	\item compute the SVD \eqref{eq:SVD}; \hfill $\mathcal{O}(k_ck_o^2+k_c^3)$
	
	\item compute the reduced-order matrices \eqref{eq:RedMatrPar}.
	\hfill $2q^2 q_A k_c k_o +2q q_B k_o m+2q q_C k_c p$\\ \hspace*{6cm} $+r(k_c+k_o)^2+r(k_ck_o+k_cp+k_om+rk_c)$
	
\end{itemize}

One can see that the computational complexity of the online stage except for the first step is independent of $n$. 


\section{Manifold $\mathcal{S}_+(k,n)$ and its interpolation scheme}\label{Sec:Manifold}
\subsection{A quotient geometry of $\mathcal{S}_+(k,n)$}
\subsection{Curve and surface interpolation for parametric model order reduction}
\subsection{A note on imbedded geometry of $\mathcal{S}_+(k,n)$}

\section{Numerical examples}\label{Sec:NumerExam}


\section{Conclusion}\label{Sec:Concl}




% BibTeX users please use one of
%\bibliographystyle{spbasic}      % basic style, author-year citations
\bibliographystyle{spmpsci}      % mathematics and physical sciences
%\bibliographystyle{spphys}       % APS-like style for physics
\bibliography{references}   % name your BibTeX data base

% Non-BibTeX users please use
%\begin{thebibliography}{}
%
% and use \bibitem to create references. Consult the Instructions
% for authors for reference list style.
%
%\bibitem{RefJ}
%% Format for Journal Reference
%Author, Article title, Journal, Volume, page numbers (year)
%% Format for books
%\bibitem{RefB}
%Author, Book title, page numbers. Publisher, place (year)
%% etc

\end{document}
% end of file template.tex

