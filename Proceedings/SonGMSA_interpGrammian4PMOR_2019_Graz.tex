%%%%%%%%%%%%%%%%%%%%%%% file template.tex %%%%%%%%%%%%%%%%%%%%%%%%%
%
% This is a general template file for the LaTeX package SVJour3
% for Springer journals.          Springer Heidelberg 2010/09/16
%
% Copy it to a new file with a new name and use it as the basis
% for your article. Delete % signs as needed.
%
% This template includes a few options for different layouts and
% content for various journals. Please consult a previous issue of
% your journal as needed.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% First comes an example EPS file -- just ignore it and
% proceed on the \documentclass line
% your LaTeX will extract the file if required
%\begin{filecontents*}{example.eps}
%!PS-Adobe-3.0 EPSF-3.0
%%BoundingBox: 19 19 221 221
%%CreationDate: Mon Sep 29 1997
%%Creator: programmed by hand (JK)
%%EndComments
%gsave
%newpath
%  20 20 moveto
%  20 220 lineto
% 220 220 lineto
%  220 20 lineto
%closepath
%2 setlinewidth
%gsave
%  .4 setgray fill
%grestore
%stroke
%grestore
%\end{filecontents*}
%
%\RequirePackage{fix-cm}
%
%\documentclass{svjour3}                     % onecolumn (standard format)
\documentclass[smallcondensed]{svjour3}     % onecolumn (ditto)
%% Text Formatting
%% Manuscripts should be submitted in LaTeX. Please use Springer�s 
%% LaTeX macro package and choose the formatting option �smallcondensed�.
%\documentclass[smallextended]{svjour3}       % onecolumn (second format)
%\documentclass[twocolumn]{svjour3}          % twocolumn
%
\smartqed  % flush right qed marks, e.g. at end of proof
%
\usepackage{graphicx}
%
% \usepackage{mathptmx}      % use Times fonts if available on your TeX system
%
% insert here the call for the packages your document requires
%\usepackage{latexsym}
% etc.
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{float}
\usepackage{color}
\usepackage{algorithm}
\usepackage{algorithmic}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
%
% please place your own definitions here and don't use \def but
% \newcommand{}{}
%
% Insert the name of "your journal" with
%\journalname{Numerical Algorithms} %Adv Comput Math
\journalname{Springer International Series of Numerical Mathematics??}
%
\begin{document}

\title{Balanced truncation for parametric linear systems
	using interpolation of gramians: a comparison of
	linear algebraic and geometric approaches}
%\subtitle{Application for a research assistant position at ICTEAM, UC Louvain}
%\thanks{}
%\thanks{Grants or other notes
%about the article that should go on the front page should be
%placed here. General acknowledgments should be placed at the end of the article.}
%\subtitle{Do you have a subtitle?\\ If so, write it here}

\titlerunning{Algebraic and Geometric interpolation of gramians for PMOR}        % if too long for running head

\author{SGMSA}  %etc.}

%\authorrunning{Short form of author list} % if too long for running head

%\institute{N. T. Son \at
%             INMA, ICTEAM, Universit/'e catholique de Louvain, 
%             Avenue Georges Lemaître 4-6/L4.05.01,\\
%             1348 Louvain-la-Neuve
%             \\
%              Tel: +32 10 47 80 10 \\
%             \email{thanh.son.nguyen@uclouvain.be}  \\%  if needed
%}

%\date{Received: date / Accepted: date}
% The correct dates will be entered by the editor


\maketitle

\begin{abstract}
In balanced truncation model order reduction, one has to solve a pair of Lyapunov equations for the two gramians and uses them for constructing a reduced-order
model. Although advances in solving such equations have been made, it is still the most
expensive step in this reduction method. For systems that depend on parameters, parametric model order reduction has to deal with the dependence on  parameters simultaneously with approximation of the input-output behavior of the full-order system. The
use of interpolation in parametric model order reduction has become popular. Nevertheless, interpolation of gramians is rarely mentioned, most probably due to the restriction to symmetric
positive semi-definite matrices. In this talk, we will present two approaches for interpolating
these structured matrices which are based on linear algebra and a recently developed Riemannian geometry. The result is then utilized in constructing parametric reduced-order
systems. Their numerical performances are compared on different models
\keywords{
Parametric model order reduction \and Balanced truncation \and Interpolation \and Gramians \and Riemannian matrix manifold \and Symmetric positive semi-definite matrices of fixed rank 
}
% \PACS{PACS code1 \and PACS code2 \and more}
\subclass{
%15A22    % Matrix pencils 
%\and
 65D05 % Interpolation
\and 65F30 % Other matrix algorithms
\and 93C05 % Control systems, Linear systems
%\and 94C % Circuits, networks
\and notdoneyet
}
\end{abstract}


\section{Introduction}\label{Sec:Intro}
The need for increasingly accurate simulations in science and
technology results in large-scale mathematical models. Simulation of those systems is usually time-consuming or even infeasible, especially with limited computer resources. Model order reduction (MOR) is well known as a tool to
deal with such problems. Founded and continuously developed already for a couple of decades, this field is still getting attraction due to the fact that many complicated or large problems have not been considered and many advanced methods have not been invoked. 

In many cases, the full order model (FOM) depends on parameters. The reduced-order model (ROM), preferably parameter-dependent as well, is therefore required to approximate the FOM on a given parameter domain. This problem, so-called parametric MOR (PMOR), has been addressed by various approaches that are based on Krylov subspaces [cite], interpolation, optimization [cite], just to name a few. The reader is referred to the survey \cite{BennGW15} for more details. Of our interest in this report is the methods that use interpolation for the linear parametric system of the form
\begin{equation}
\label{eq:psys}
\arraycolsep=2pt
\begin{array}{rcl}
E(\mu)\dot{x}(t,\mu) & = &A(\mu)x(t,\mu)+B(\mu)u(t),\\
y(t,\mu) & = & C(\mu)x(t,\mu),
\end{array}
\end{equation}
where $E(\mu)$, $A(\mu) \in \mathbb{R}^{n\times n}$, $B(\mu) \in \mathbb{R}^{n\times m}$, 
$C(\mu) \in \mathbb{R}^{p\times n}$ with $p,m \ll n$, and $\mu\in\Omega\subset \mathbb{R}^d$. We assume that the matrix 
$E(\mu)$ is nonsingular and all eigenvalues of the pencil $\lambda E(\mu)-A(\mu)$ have negative real part for all $\mu\in\Omega$. The goal is to approximate system \eqref{eq:psys} with a parametric model
\begin{equation}
\label{eq:redpsys}
\arraycolsep=2pt
\begin{array}{rcl}
\tilde{E}(\mu)\dot{\tilde{x}}(t,\mu) & =\tilde{A}(\mu)\tilde{x}(t,\mu)+\tilde{B}(\mu)u(t),\\
\tilde{y}(t,\mu) & =\tilde{C}(\mu)\tilde{x}(t,\mu),
\end{array}
\end{equation}
where $\tilde{E}(\mu)$, $\tilde{A}(\mu) \in \mathbb{R}^{r\times r}$, $\tilde{B}(\mu)\in\mathbb{R}^{r\times m}$, 
$\tilde{C}(\mu) \in \mathbb{R}^{p\times r}$ and $r \ll n$. 

The idea of interpolation is straightforward. On a given grid $\mu_i$, $i = 1, \dots, q$ in the parameter domain $\Omega$, one computes a ROM associated with each $\mu_i$. These ROMs can be computed using any MOR method for non-parametric models \cite{Anto05} and characterized by either their  projection subspaces, coefficient matrices, or transfer functions. Then, they are interpolated using standard methods. These topics have been discussed intensively in many publications with applications to various fields, see, e.g., \cite{AmsaF08,BaurB09,PanzMEL10,DegrVW10,AmsaF11,Son12,SonS15}. Each of them has its own strength and acts well in some specific applications but fails to be superior to the others. 

When balanced truncation \cite{Moor81} is used,  one has to solve a pair of Lyapunov equations for the two gramians. Although advances in solving such equations have been made, it is still the most expensive step in this reduction method. Therefore, any interpolation method that can circumvent this step is of interest. We suggest to interpolate the solution of these equations, so-called gramians. It is noteworthy that in large-scale setting, one should never work with full-rank solution matrices. Fortunately, in many practical cases, Lyapunov equaions accept low-rank symmetric positive semi-definite (spsd) approximation \cite{Penz00b,AntoSZ02}. It not only makes the computation more efficient but also enables the squaring procedure in balanced truncation \cite{TombP87}. A resulting difficulty arises in the interpolation approach is that the interpolant is also expected to be spsd. That is, the spsd property must be preserved during the interpolation. To this end, we propose in this paper two approaches. In the first one, we first invoke some positive interpolation scheme to preserve the semi-definiteness and then %use some compression technique to keep the interpolant low-rank. After that, 
design an offline-online decomposition so that the interpolation is deeply ``embedded" in both stages through which the online stage is accelerated. We refer to this as linear algebraic approach. The second one is based completely on differential geometry, so the name. It was shown in \cite{VandAV09,MassA18} that the set of spsd matrices of fixed rank can be turned into a Riemannian manifold by equipping it with a differential structure. As a result, if all solutions of the Lyapunov equations at grid points are approximated by spsd matrices of a prescribed low rank, we then encounter interpolation on a Riemannian manifold. Based on a recent result \cite{MassA18}, we will explain in detail how to do it.

The rest of the paper is organized as follows. In section~\ref{Sec:BT_standard interpolation} we first briefly recall the squaring procedure for balancing equation and then present in detail how interpolate the gramians while preserve its low-rank semi-positiveness and how to prepare data in the offline step so that we can speed up the online step. A quotient geometry for the Riemannian manifold of spsd matrices of fixed rank is constructed in the first part of section~\ref{Sec:Manifold}. We then explain in detail how to interpolate on this manifold using the tools just developed. We also discuss the possibility of using a embedded geometry  for this talk in this section. The proposed approaches are illustrated by numerical examples in section~\ref{Sec:NumerExam} in which we also compare their behaviors and numerical efficiency. Conclusion is given in section~\ref{Sec:Concl}. 

Throughout this paper, we will use ... for the nations.





\section{Brief balanced truncation for parametric linear systems and standard interpolation}\label{Sec:BT_standard interpolation}
\subsection{Balanced truncation}
As other projection-based method, a balancing projection for system \eqref{eq:psys} must be constructed. To this end, one has to solve a pair of the generalized Lyapunov equations
\begin{eqnarray}
\label{eq:LyapContr}
E(\mu)P(\mu)A^T\!(\mu) + A(\mu)P(\mu)E^T\!(\mu) & = & -B(\mu)B^T\!(\mu), \\
\label{eq:LyapObser}
E^T(\mu)Q(\mu)A(\mu) + A^T(\mu)Q(\mu)E(\mu) & = & -C^T(\mu)C(\mu),
\end{eqnarray}
for the \textit{controllability Gramian} $P(\mu)$ and the \textit{observability Gramian} $Q(\mu)$. In practice, 
these Gramians are computed in the factored form 
$$
P(\mu) = X(\mu)X^T(\mu), \qquad Q(\mu) = Y(\mu)Y^T(\mu)
$$ 
with $X(\mu)\in\mathbb{R}^{n\times k_c}$ and $Y(\mu)\in\mathbb{R}^{n\times k_o}$. One can show that eigenvalues of the matrix $P(\mu)E^T(\mu)Q(\mu)E(\mu)$ are real and non-negative. The square roots of the eigenvalues of this matrix $\sigma_1(\mu) \geq \cdots \geq \sigma_{n}(\mu)\geq 0$ are called the \textit{Hankel singular values} of system \eqref{eq:psys}. Consider the singular value decomposition 
\begin{align}
\label{eq:SVD}
Y^T(\mu)E(\mu)X(\mu) = [U_1(\mu)\enskip U_0(\mu)]\begin{bmatrix}
\Sigma_1(\mu) & 0\\ 0 & \Sigma_0(\mu)
\end{bmatrix} [V_1(\mu)\enskip V_0(\mu)]^T,
\end{align}
where $[U_1(\mu)\enskip U_0(\mu)]$ and $[V_1(\mu)\enskip V_0(\mu)]$ are orthogonal, and
$$
\Sigma_1(\mu) = \mbox{diag}(\sigma_1(\mu),\ldots,\sigma_r(\mu)), \quad 
\Sigma_0(\mu) = \mbox{diag}(\sigma_{r+1}(\mu),\ldots,\sigma_{k_{co}}(\mu))
$$
with $k_{co}=\min(k_c,k_o)$.
Then the reduced-order model is computed by projection 
\begin{equation}\label{eq:RedMatr}
\begin{array}{ll}
\tilde{E}(\mu) = W^T(\mu)E(\mu)T(\mu),& \quad \tilde{A}(\mu) = W^T(\mu)A(\mu)T(\mu), \\
\tilde{B}(\mu) = W^T(\mu)B(\mu), & \quad \tilde{C} = C(\mu)T(\mu),
\end{array}
\end{equation}
where the projection matrices are given by
\begin{equation}
W(\mu) = Y(\mu)U_1(\mu) \Sigma_1^{-1/2}(\mu), \qquad T(\mu) = X(\mu)V_1(\mu)\Sigma_1^{-1/2}(\mu).
\label{eq:WT}
\end{equation}
The error of the approximation is shown to satisfy 
\begin{equation*}%\label{S2ErrBound}
\|H(\cdot,\mu)-\tilde{H}(\cdot,\mu)\|_{\mathcal{H}_\infty} \leq 2\bigl(\sigma_{r+1}(\mu) + \cdots + \sigma_{k_{co}}(\mu)\bigr),
\end{equation*}
where 
\begin{align*}
H(s,\mu)& =C(\mu)(sE(\mu)-A(\mu))^{-1}B(\mu), \\
\tilde{H}(s,\mu)&=\tilde{C}(\mu)(s\tilde{E}(\mu)-\tilde{A}(\mu))^{-1}\tilde{B}(\mu)
\end{align*}
are the transfer functions of systems \eqref{eq:psys} and \eqref{eq:redpsys}, respectively. 
\subsection{Interpolation of gramians for parametric model order reduction}
On the chosen grid $\mu_1,\ldots,\mu_q \in \Omega$, we solve the equations (\ref{eq:LyapContr}) and (\ref{eq:LyapObser}) 
for $P_j=X_j^{}X_j^T$ %\in\mathcal{S}_+(k_c,n)$ 
and $Q_j=Y_j^{}Y_j^T$
%\in\mathcal{S}_+(k_o,n)$,
$j=1,\ldots,q$. Note that the ranks of local gramians $P_j, j=1,\ldots,q$ and $Q_j, j=1,\ldots,q$ need not be the same. Then the parameter-dependent gramians can be approximated by interpolation as
\begin{equation*}
P(\mu) = \sum\limits_{j=1}^qw_j(\mu)X_j^{}X_j^T,\quad Q(\mu) = \sum\limits_{j=1}^qw_j(\mu)Y_j^{}Y_j^T,
%&X_{im}(p) = \sum\limits_{j=1}^kw_j(p)X^j_{im},\quad Y_{im}(p) = \sum\limits_{j=1}^kw_j(p)Y^j_{im},
\end{equation*}
where $w_j(\mu)$ are some weights. To preserve the semi-definiteness of gramians, we propose to use non-negative weights \cite{Alla03}. Moreover, it allows us to retain the factorization structure. To wit
\begin{align}\label{S3GramparaLR}
%\begin{split}
P(\mu) = 
&= \sum\limits_{j=1}^q\sqrt{w_j(\mu)}X_{j}\sqrt{w_j(\mu)}X^{T}_{j}\\ \notag
&=\begin{bmatrix}
\sqrt{w_1(\mu)}X_{1}&\cdots & \sqrt{w_q(\mu)}X_{q}
\end{bmatrix}
\begin{bmatrix}
\sqrt{w_1(\mu)}X^{T}_{1}\\ \cdots \\ \sqrt{w_q(\mu)}X^{T}_{q}
\end{bmatrix}\\ \notag
&=: X(\mu)X^T(\mu).
%\end{split}
\end{align}
Likewise, 
\begin{equation}
	Q(\mu) = Y(\mu)Y^T(\mu).
\end{equation}

Note that computation of parametric gramians is not the ultimate goal of the task. After interpolation, we still have to proceed steps \eqref{eq:SVD} and \eqref{eq:RedMatr} to get the reduced-order model; computation explicitly involves large matrices which may reduce the efficiency of the proposed method. %Meanwhile, due to the simulation requirement of parameter dependent systems, the online stage should have the computational complexity independent of $n$.
A solution to overcome this is to rigorously separate all computation into two stages. The first stage can be expensive but must be independent of $\mu$ so that it can serve as a preparation step and the derived data can be used for any value of $\mu$.  In the second step, where one has to compute the ROM at any new value of $\mu$ in the parameter domain, must be fast. A criterion for being fast is that its computational complexity is independent of large order $n$. This approach is mentioned as offline-online decomposition and quite well-known in the reduced basis community \cite{PateR07,HeRS16}. Details are presented in the next subsection. %The advantage of this algorithm is that expensive and time consuming steps can be computed and the data can be stored for the online stage. The online stage, counted from the moment when a value of $p$ is given till the moment that the reduced order is computed, has computational complexity independent of $n$ and therefore can be used in real time \cite{Son13}. Unfortunately, this decomposition can be applied only to the case that $E(p)$ is non-singular. The explanation for this fact and the detail of the method is given in the next subsection.

\subsection{Offline-online decomposition}
To enable the online-offline decomposition, we need an assumption that the matrices of system (\ref{eq:psys}) depend affinely on the parameter $p$. More precisely,
\begin{align*}%\label{S3AffDep}
&E(\mu) = \sum\limits_{i=1}^{q_E}f_i^E(\mu)E_i,\quad A(\mu) = \sum\limits_{i=1}^{q_A}f_i^A(\mu)A_i,\\
&B(\mu) = \sum\limits_{i=1}^{q_B}f_i^B(\mu)B_i, \quad C(\mu) = \sum\limits_{i=1}^{q_C}f_i^C(\mu)C_i,
\end{align*}
where $q_E, q_A, q_B, q_C$ are small and the evaluations of $f_i^E,f_i^A,f_i^B,f_i^C$ are cheap. %Recalling that when $E$ is non-singular, no projection on the deflating subspaces and the improper gramians are needed. 
Once the interpolated gramians are available, % assuming that all low-rank factors have $l_r$ columns,
 it follows that
\begin{align}\notag
&Y_{}^T(\mu)E(\mu)X_{}(\mu) = \begin{bmatrix}
\sqrt{w_1(\mu)}Y^{T}_{1}\\ \cdots \\ \sqrt{w_q(\mu)}Y^{T}_{q}
\end{bmatrix} \sum\limits_{i=1}^{q_E}f_i^E(\mu)E_i \begin{bmatrix}
\sqrt{w_1(\mu)}X_{1}&\cdots & \sqrt{w_q(\mu)}X_{q}
\end{bmatrix}\\ \label{S3SVDpara}
&= \sum\limits_{i=1}^{q_E}f_i^E(\mu) \begin{bmatrix}
w_1(\mu)Y^{T}_{1}E_iX_{1} &\cdots & \sqrt{w_1(\mu)w_q(\mu)}Y^{T}_{1}E_iX_{q}\\
\vdots & \vdots & \vdots \\
\sqrt{w_k(\mu)w_1(\mu)}Y^{T}_{q}E_iX_{1} & \cdots & 
w_k(p)Y^{T}_{q}E_iX_{q}
\end{bmatrix}.
\end{align}
Obviously, all $q_Eq^2$ blocks $Y^{T}_{\alpha}E_iX_{\beta} %\in \mathbb{R}^{l_r\times l_r},
\quad \alpha, \beta = 1,\cdots,q, i=1,\cdots,q_E,$ can be computed, stored and used for any values of $\mu$ since they are independent of $\mu$. After computing the SVD of (\ref{S3SVDpara}) as in \eqref{eq:SVD}, the projection matrices for the reduction are given by
\begin{align*}%\label{S3PrMat}
&W^T(\mu) = \Sigma_1^{-1/2}(\mu)U_1^T(\mu)\begin{bmatrix}
\sqrt{w_1(\mu)}Y^{T}_{1}\\ \cdots \\ \sqrt{w_q(\mu)}Y^{T}_{q}
\end{bmatrix},\\
& T(p) = \begin{bmatrix}
\sqrt{w_1(\mu)}X_{1}&\cdots & \sqrt{w_k(\mu)}X_{q}
\end{bmatrix} V_1(p) \Sigma_1^{-1/2}(p). 
\end{align*}
The reduced matrices are then computed in the same manner like (\ref{S3SVDpara})
\begin{align}\notag
&\tilde{E}(\mu) = W^T(\mu)E(\mu)T(\mu)= \sum\limits_{i=1}^{q_E}f_i^E(\mu)\Sigma_1^{-1/2}(\mu)U_1^T(\mu)\times\\ \label{S3RedMatPara1}
&\begin{bmatrix}
w_1(\mu)Y^{T}_{1}E_iX_{1} &\cdots & \sqrt{w_1(\mu)w_q(\mu)}Y^{T}_{1}E_iX_{q}\\
\vdots & \vdots & \vdots \\
\sqrt{w_k(\mu)w_1(\mu)}Y^{T}_{q}E_iX_{1} & \cdots & 
w_k(\mu)Y^{T}_{q}E_iX_{q}
\end{bmatrix}V_1(\mu) \Sigma_1^{-1/2}(\mu),\\ \notag
&\tilde{A}(\mu) = W^T(\mu)A(\mu)T(\mu)= \sum\limits_{i=1}^{q_A}f_i^A(\mu)\Sigma_1^{-1/2}(\mu)U_1^T(\mu)\times\\
&\begin{bmatrix}
w_1(\mu)Y^{T}_{1}A_iX_{1} &\cdots & \sqrt{w_1(\mu)w_q(\mu)}Y^{T}_{1}A_iX_{q}\\
\vdots & \vdots & \vdots \\
\sqrt{w_k(\mu)w_1(\mu)}Y^{T}_{q}A_iX_{1} & \cdots & 
w_k(\mu)Y^{T}_{q}A_iX_{q}
\end{bmatrix}V_1(\mu) \Sigma_1^{-1/2}(\mu),\\ 
&\tilde{B}(\mu) = W^T(\mu)B()\mu)= \sum\limits_{i=1}^{q_B}f_i^B(\mu)\Sigma_1^{-1/2}(\mu)U_1^T(\mu)
\begin{bmatrix}
\sqrt{w_1(\mu)}Y^{T}_{1}B_i\\
\vdots \\
\sqrt{w_q(\mu)}Y^{T}_{q}B_i 
\end{bmatrix},\\
&\tilde{C}(\mu) = C(\mu)T(\mu) \notag \\
&= \sum\limits_{i=1}^{q_C}f_i^C(\mu)[
\sqrt{w_1(\mu)}C_iX_{1} \cdots  \sqrt{w_q(\mu)}C_iX_{q}]V_1(\mu)\Sigma_1^{-1/2}(\mu).\label{S3RedMatPara2}
\end{align}
Again, all matrix blocks, that are independent of $\mu$, can be computed and stored before hand. Based on the above analyses, the offline-online procedure is given below.
\begin{description}
	\item[\textbf{Offline}]Do the following
	\begin{itemize}
		\item Solve Lyapunov equations \eqref{eq:LyapContr} and \eqref{eq:LyapObser} for all $X_{j}$ and $Y_{j}, j = 1,\cdots,q$.
		\item Compute and store all the parameter independent matrix blocks mentioned in (\ref{S3SVDpara})-(\ref{S3RedMatPara2}).
	\end{itemize}
	\item[\textbf{Online}] Given a value $\mu$ for the parameter
	\begin{itemize}
		\item Assemble precomputed matrix blocks and compute the SVD of (\ref{S3SVDpara}).
		\item Assemble precomputed matrix blocks and compute the reduced matrices (\ref{S3RedMatPara1})-(\ref{S3RedMatPara2}). 
	\end{itemize}
\end{description}
Assume that all approximate solutions of Lyapunov equations have rank $l_r$, ene can see that the computational complexity of the online stage is scaled with $\mathcal{O}(rk^2l_r^2)$.









\section{Manifold $\mathcal{S}_+(k,n)$ and its interpolation scheme}\label{Sec:Manifold}
\subsection{A quotient geometry of $\mathcal{S}_+(k,n)$}
\subsection{Curve and surface interpolation for parametric model order reduction}
\subsection{A note on imbedded geometry of $\mathcal{S}_+(k,n)$}

\section{Numerical examples}\label{Sec:NumerExam}


\section{Conclusion}\label{Sec:Concl}




% BibTeX users please use one of
%\bibliographystyle{spbasic}      % basic style, author-year citations
\bibliographystyle{spmpsci}      % mathematics and physical sciences
%\bibliographystyle{spphys}       % APS-like style for physics
\bibliography{references}   % name your BibTeX data base

% Non-BibTeX users please use
%\begin{thebibliography}{}
%
% and use \bibitem to create references. Consult the Instructions
% for authors for reference list style.
%
%\bibitem{RefJ}
%% Format for Journal Reference
%Author, Article title, Journal, Volume, page numbers (year)
%% Format for books
%\bibitem{RefB}
%Author, Book title, page numbers. Publisher, place (year)
%% etc

\end{document}
% end of file template.tex

